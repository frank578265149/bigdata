<?xml version="1.0" encoding="UTF-8"?>
<configuration supports_final="true">
    <property>
        <name>spark.yarn.queue</name>
        <value>default</value>
        <description>
            The name of the YARN queue to which the application is submitted.
        </description>
        <depends-on>
            <property>
                <type>capacity-scheduler</type>
                <name>yarn.scheduler.capacity.root.queues</name>
            </property>
        </depends-on>
        <on-ambari-upgrade add="false"/>
    </property>
    <property>
        <name>spark.driver.extraLibraryPath</name>
        <value>{{spark_hadoop_lib_native}}</value>
        <description>
            â€‚Set a special library path to use when launching the driver JVM.
        </description>
        <on-ambari-upgrade add="true"/>
    </property>
    <property>
        <name>spark.executor.extraLibraryPath</name>
        <value>{{spark_hadoop_lib_native}}</value>
        <description>
            Set a special library path to use when launching the executor JVM.
        </description>
        <on-ambari-upgrade add="true"/>
    </property>
    <property>
        <name>spark.history.provider</name>
        <value>org.apache.spark.deploy.history.FsHistoryProvider</value>
        <description>Name of history provider class</description>
        <on-ambari-upgrade add="true"/>
    </property>
    <property>
        <name>spark.history.fs.logDirectory</name>
        <value>{{spark_history_dir}}</value>
        <final>true</final>
        <description>
            Base directory for history spark application log. It is the same value
            as in spark-defaults.xml.
        </description>
        <on-ambari-upgrade add="true"/>
    </property>
    <property>
        <name>spark.eventLog.enabled</name>
        <value>true</value>
        <final>true</final>
        <description>
            Whether to log Spark events, useful for reconstructing the Web UI after the application has finished.
        </description>
        <on-ambari-upgrade add="true"/>
    </property>
    <property>
        <name>spark.eventLog.dir</name>
        <value>{{spark_history_dir}}</value>
        <final>true</final>
        <description>
            Base directory in which Spark events are logged, if spark.eventLog.enabled is true. It is the same value
            as in spark-defaults.xml.
        </description>
        <on-ambari-upgrade add="true"/>
    </property>
    <property>
        <name>spark.master</name>
        <value>{{spark_thrift_master}}</value>
        <description>
            The deploying mode of spark application, by default it is yarn-client for thrift-server but local mode for
            there's
            only one nodemanager.
        </description>
        <on-ambari-upgrade add="true"/>
    </property>
    <property>
        <name>spark.scheduler.allocation.file</name>
        <value>{{spark_conf}}/spark-thrift-fairscheduler.xml</value>
        <description>
            Scheduler configuration file for thriftserver.
        </description>
        <on-ambari-upgrade add="true"/>
    </property>
    <property>
        <name>spark.scheduler.mode</name>
        <value>FAIR</value>
        <description>
            The scheduling mode between jobs submitted to the same SparkContext.
        </description>
        <on-ambari-upgrade add="true"/>
    </property>
    <property>
        <name>spark.shuffle.service.enabled</name>
        <value>true</value>
        <description>
            Enables the external shuffle service.
        </description>
        <on-ambari-upgrade add="true"/>
    </property>
    <property>
        <name>spark.hadoop.cacheConf</name>
        <value>false</value>
        <description>
            Specifies whether HadoopRDD caches the Hadoop configuration object
        </description>
        <on-ambari-upgrade add="true"/>
    </property>
    <property>
        <name>spark.dynamicAllocation.enabled</name>
        <value>true</value>
        <description>
            Whether to use dynamic resource allocation, which scales the number of executors registered with this
            application up and down based on the workload.
        </description>
        <on-ambari-upgrade add="true"/>
    </property>
    <property>
        <name>spark.dynamicAllocation.initialExecutors</name>
        <value>1</value>
        <description>
            Initial number of executors to run if dynamic allocation is enabled.
        </description>
        <on-ambari-upgrade add="true"/>
    </property>
    <property>
        <name>spark.dynamicAllocation.maxExecutors</name>
        <value>1000</value>
        <description>
            Upper bound for the number of executors if dynamic allocation is enabled.
        </description>
        <on-ambari-upgrade add="true"/>
    </property>
    <property>
        <name>spark.dynamicAllocation.minExecutors</name>
        <value>1</value>
        <description>
            Lower bound for the number of executors if dynamic allocation is enabled.
        </description>
        <on-ambari-upgrade add="true"/>
    </property>

    <property>
        <name>spark.streaming.backpressure.enabled</name>
        <value>true</value>
        <description>
            Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark
            Streaming to control the receiving rate based on the current batch scheduling delays and processing times so
            that the system receives only as fast as the system can process. Internally, this dynamically sets the
            maximum receiving rate of receivers. This rate is upper bounded by the values
            spark.streaming.receiver.maxRate and spark.streaming.kafka.maxRatePerPartition if they are set
        </description>
        <on-ambari-upgrade add="true"/>
    </property>
    <property>
        <name>spark.streaming.backpressure.initialRate</name>
        <value>10000</value>
        <description>
            This is the initial maximum receiving rate at which each receiver will receive data for the first batch when
            the backpressure mechanism is enabled.
        </description>
        <on-ambari-upgrade add="true"/>
    </property>
    <property>
        <name>spark.streaming.receiver.maxRate</name>
        <value>30000</value>
        <description>
            Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each
            stream will consume at most this number of records per second. Setting this configuration to 0 or a negative
            number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for
            mode details.
        </description>
        <on-ambari-upgrade add="true"/>
    </property>
    <property>
        <name>spark.streaming.kafka.maxRatePerPartition</name>
        <value>30000</value>
        <description>
            Maximum rate (number of records per second) at which data will be read from each Kafka partition when using
            the new Kafka direct stream API. See the Kafka Integration guide for more details.
        </description>
        <on-ambari-upgrade add="true"/>
    </property>
    <property>
        <name>spark.streaming.stopGracefullyOnShutdown</name>
        <value>true</value>
        <description>
            If true, Spark shuts down the StreamingContext gracefully on JVM shutdown rather than immediately.
        </description>
        <on-ambari-upgrade add="true"/>
    </property>
    <property>
        <name>spark.serializer</name>
        <value>org.apache.spark.serializer.KryoSerializer</value>
        <description>
            Class to use for serializing objects that will be sent over the network or need to be cached in serialized
            form. The default of Java serialization works with any Serializable Java object but is quite slow, so we
            recommend using org.apache.spark.serializer.KryoSerializer and configuring Kryo serialization when speed is
            necessary. Can be any subclass of org.apache.spark.Serializer.
        </description>
        <on-ambari-upgrade add="true"/>
    </property>
</configuration>
